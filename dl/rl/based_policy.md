##  Reinforcement Learning

### **关键词:** 
RL  优化策略网络

### **摘要:** 
策略梯度算法是 RL 中一种以优化策略网络以最大化累计奖励的算法

### **情形**

1.  **基本原理**

通过计算策略网络参数的微小变化如何影响累计奖励，从而找到使累计奖励最大化的策略
它利用采样得到的轨迹数据，计算每个动作的优势估计，进而估计策略梯度

2. **算法流程**

- 策略表示：使用参数化的策略网络 $\pi_{\theta}(a|s)$ 来表示在状态 s 下采取动作 a 的概率，其中 $\theta$ 是策略网络的参数。
 
- 采样：从当前策略 $\pi_{\theta}$ 中采样得到一系列的轨迹 $\tau=(s_0,a_0,r_0,s_1,a_1,r_1,\cdots)$，其中 $s_t$ 是时刻 t 的状态，$a_t$ 是采取的动作，$r_t$ 是获得的奖励。
 
- 计算奖励和优势：计算每个轨迹的累计奖励 $R(\tau)$ ，并估计每个动作的优势 $A(s_t,a_t)$，优势表示该动作相对于平均策略的好坏程度。
 
- 计算策略梯度：根据采样数据和优势估计，计算策略梯度 $\nabla_{\theta}J(\theta)$，其中 $J(\theta)$ 是策略的目标函数，通常是期望累计奖励。
 
- 更新策略参数：使用计算得到的策略梯度来更新策略网络的参数，例如通过随机梯度下降法$\theta\leftarrow\theta+\alpha\nabla_{\theta}J(\theta)$ ，其中 $\alpha$ 是学习率。

<details>
    <summary>补充</summary>
        <ul>
	      <li><strong>.</strong>： .</li>
        </ul>
</details>

